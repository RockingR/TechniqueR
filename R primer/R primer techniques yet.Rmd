---
title: "R primer techniques yet"
author: "Kenny"
date: "Friday, January 09, 2015"
output: html_document
---


### Read data from an SQL database using ODBC
```{r}
library(RODBC)
# Connect to SQL database with username and password
channel <- odbcConnect("mydata", uid="tv", pwd="office") 
sqlTables(channel)               # List tables in the database
mydata <- sqlFetch(channel, "paper")      # Fetch entire table
mydata
sqlQuery(channel, 
         "SELECT * FROM paper WHERE Sales>12 ORDER BY Person")
odbcClose(channel)
```


### Read data from a simple XML file

```{r}
library(XML)
url <- "http://www.statistics.life.ku.dk/primer/mydata.xml"
indata <- xmlToDataFrame(url)
head(indata)
```

### Read data from an XML file

```{r}
library(XML)
# Location of the example XML file
url <- "http://www.statistics.life.ku.dk/primer/bank.xml"
# Parse the tree 
doc <- xmlTreeParse(url, useInternalNodes=TRUE)
top <- xmlRoot(doc)         # Identify the root node
xmlName(top)                # Show node name of root node
names(top)                  # Name of root node children
xmlValue(top[[1]])          # Access first element 
xmlValue(top[["author"]])   # First element with named index
names(top[[3]])             # Children of node 3
xmlAttrs(top[[3]][[1]])     # Extract tags from a Date node
top[["rates"]][[1]][[1]]    # Tree from first bank and date
xmlValue(top[[3]][[1]][[1]][[1]]) # Bank name is node value
xmlAttrs(top[[3]][[1]][[1]][[1]]) # but has no tags
xmlAttrs(top[[3]][[1]][[1]][[2]]) # The  node has tags
xmlValue(top[[3]][[1]][[1]][[2]]) # but no value
# Search tree for all source nodes and return their value
xpathSApply(doc, "//source", xmlValue)
# Search full tree for all exch nodes where currency is "DKK"
xpathApply(doc, "//exch[@currency='DKK']", xmlAttrs)
res <- xpathApply(doc, "//exch",  
         function(ex) {
            c(xmlAttrs(ex),
              bank=xmlValue(xmlParent(ex)[["source"]]),
              date=xmlGetAttr(xmlParent(xmlParent(ex)), "time"))
           })
result <- do.call(rbind, res)
result
```



### Export a data frame to a SAS dataset
```
library(SASxport)
data(trees)
write. xport(trees, file="datafromR. xpt")
```
### Export a data frame to an SPSS dataset
```
library(foreign)
data(trees)
write. foreign(trees, datafile="mydata. dat", codefile="mydata.sps", package="SPSS")
```

### Export a data frame to a Stata dataset
```
library(foreign)
data(trees)
write.dta(trees, file="mydata.dta")
```


### Export a data frame to XML
```
library(XML)
data(trees)
mydata <- trees # Use cherry trees data as example
doc <- newXMLDoc() # Start empty XML document tree
# Start by adding a document tag at the root of the XML file
root <- newXMLNode("document", doc=doc)
invisible( # Make output invisible
lapply(1: nrow(mydata) , # Iterate over all rows
function(rowi) {
r <- newXMLNode("row", parent=root) # Create row tag
for(var in names(mydata) ) { # Iterate over variables
# Add each observation
newXMLNode(var, mydata[rowi, var] , parent = r)
} } ) )
# Now save the XML document tree as file mydata. xml
saveXML(doc, file="mydata. xml")
```



## Importing data from other statistical software programs
### Import a SAS dataset
```
library(foreign)
indata <- read.xport("somefile.xpt") # save data in .xpt first
```

### Import an SPSS dataset
```
library(foreign)
indata <- read.spss("spssfilename.sav", to.data.frame = TRUE) 
```

### Import a Stata dataset
```
library(foreign)
indata <- read.dta("statafile.dta")
```

### Import a Systat dataset
```
library(foreign)
indata <- read. systat("systatfile. syd") 

```



### Use a Box-Cox transformation to make non-normally distributed data approximately normal

```{r}
library(MASS)
data(trees)
result <- boxcox(Volume ~ log(Height) + log(Girth), data=trees,
                 lambda=seq(-0.5, 0.6, length=13))
# Find the lambda value with highest profile log-likelihood
result$x[which.max(result$y)] 
y <- rnorm(100) + 5       # Generate data
y2 <- y**2                # Square the results
result <- boxcox(y2 ~ 1, plotit=FALSE)
result$x[which.max(result$y)] 
```

### Calculate the area under a curve

```{r}
trapezoid <- function(x, y) { 
                      sum(diff(x)*(y[-1]+y[-length(y)]))/2 }
x <- 1:4
y <- c(0, 1, 1, 5)
trapezoid(x, y)
x2 <- c(3, 4, 2, 1)  # Reorder the same data
y2 <- c(1, 5, 1, 0)
trapezoid(x2, y2)    # We get wrong result
auc <- function(x, y, from=min(x), to=max(x), ...) {
                integrate(approxfun(x,y,...), from, to)$value }
auc(x, y)
auc(x2, y2)                           # Reordering is fine
auc(x, y, from=0)                     # AUC from 0 to max(x)
auc(x, y, from=0, rule=2)             # Allow extrapolation
auc(x, y, from=0, rule=2, yleft=0)    # Use value 0 to the left
auc(x, y, from=0, rule=2, yleft=.5)   # Use 1/2 to the left
data(ChickWeight)
result <- by(ChickWeight, ChickWeight$Chick, 
             function(df) { auc(df$Time, df$weight) } )
head(result)
# Make sure that each area are based on the same interval
# from 0 to 21 days. Carry first and last observation
# backward and forward, respectively
result2 <- by(ChickWeight, ChickWeight$Chick, function(df) { 
              auc(df$Time, df$weight, from=0, to=21, rule=2)})
head(result2)
```

statistics

Statistical analyses
Descriptive statistics
Create descriptive tables
Linear models
Fit a linear regression model
Fit a multiple linear regression model
Fit a polynomial regression model
Fit a one-way analysis of variance
Fit a two-way analysis of variance
Fit a linear normal model
Generalized linear models
Fit a logistic regression model
Fit a multinomial logistic regression model
Fit a Poisson regression model
Fit an ordinal logistic regression model
Methods for analysis of repeated measurements
Fit a linear mixed-effects model
Fit a linear mixed-effects model with serial correlation
Fit a generalized linear mixed model
Fit a generalized estimating equation model
Decompose a time series into a trend, seasonal, and residual components
Analyze time series using an ARMA model
Specific methods
Compare populations using t test
Fit a nonlinear model
Fit a Tobit regression model
Model validation
Test for normality of a single sample
Test for variance homogeneity across groups
Validate a linear or generalized linear model
Contingency tables
Analysis of two-dimensional contingency tables
Analyze contingency tables using log-linear models
Agreement
Create a Bland-Altman plot of agreement to compare two quantitative methods
Determine agreement among several methods of a quantitative measurement
Calculate Cohen's kappa
Multivariate methods
Fit a multivariate regression model
Cluster observations
Use principal component analysis to reduce data dimensionality
Fit a principal component regression model
Classify observations using linear discriminant analysis
Use partial least squares regression for prediction
Resampling statistics and bootstrapping
Non-parametric bootstrap analysis
Use cross-validation to estimate the performance of a model or algorithm
Calculate power or sample size for simple designs
Robust statistics
Correct p-values for multiple testing
Non-parametric methods
Use Wilcoxon's signed rank test to test a sample median
Use Mann-Whitney's test to compare two groups
Compare groups using Kruskal-Wallis' test
Compare groups using Friedman's test for a two-way block design
Survival analysis
Fit a Kaplan-Meier survival curve to event history data
Fit a Cox regression model (proportional hazards model)
Fit a Cox regression model (proportional hazards model) with time-varying covariates
